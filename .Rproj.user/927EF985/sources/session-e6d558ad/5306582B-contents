---
title: "Analyzing Keywords - Test Class - Quanteda Text Analysis"
author: "Rob Wells"
date: '2023-4-30'
output: html_document
---

```{r include=FALSE}
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
library(rio)
#install.packages("quanteda") 
library(quanteda)
```

# Basic Analysis

## Mob Narrative

```{r}
#Import Data
mob <- rio::import("https://docs.google.com/spreadsheets/d/1MChohD5Oy9f_71nWCx8srMh8CcktoYGF8-bGfB0uOV8/edit#gid=1436302326")

#Clean
#Code: hostile= 1; orderly= 2; neutral =3; unclear= 4, irrelevant =5; duplicate = 0
mob <- mob %>% 
  rename(code = 'Code: hostile= 1; orderly= 2; neutral =3; unclear= 4, irrelevant =5; duplicate = 0')

#Years to Decades
mob$decade <- paste0(substr(mob$year, 0, 3), "0")

#exclude 0, 4, 5: unclear= 4, irrelevant =5; duplicate = 0
```


```{r}
#Possible import / export code
# write_csv(mob, "../test_class/student_data/mob_analysis_april30.csv")

# mob1 <- rio::import("../test_class/student_data/mob_analysis_april30.csv")

```


```{r}
#Analysis
mob_sum <- mob %>% 
  group_by(code, decade) %>% 
  filter(!(code %in% c("0", "4", "5")))%>%
  filter(!is.na(code)) %>% 
  count(code) %>% 
   mutate(
     category= case_when(
        code == '1'~ "hostile",
        code == '2' ~ "orderly",
        code == '3'~ "neutral"
      )
  ) 

#normalize
mob_sum <- mob_sum %>% 
  select(code, decade, n, category) %>%  
  group_by(decade) %>% 
  mutate(decade_total = (n/sum(n))) %>% 
  ungroup()
            
    

```
Visualize
```{r}
mob_sum %>% 
ggplot(aes(x = decade, y = decade_total, fill = category)) +
  geom_col(position = "dodge") + 
    labs(title = "Mob Disposition", 
       subtitle = "Content Analysis of Mob Narrative. Pct of Category / Decade",
       caption = "Graphic by Rob Wells & Mohamed Salama, 4/9/2023",
       y="Pct of Category",
       x="Year")
```



#------------------------------------------------------
# Pro Zone!
## Code to import and create new spreadsheets 

### Import Data
```{r}
#import df created from Sean's compiler of raw text sequence - source code in appendix of this document
#formerly articles_1pct_dec26 
lynch <- read_csv("../test_class/data/articles_march8.csv")

#index of 1 pct sample which has been checked by a coder and represents all valid entries
#formerly jackindex_dec26
jackindex <- read_csv("../test_class/data/jackindex_march8.csv")

#index of all 59,967 articles captured

index <- read_csv("../test_class/data/index_feb6.csv") %>% 
  as.data.frame()

index <- janitor::clean_names(index)


```

# Tokenizing for all word counts

```{r}
all_text <- str_replace_all(lynch$sentence, "- ", "")
text_df <- tibble(all_text,)

# unnest includes lower, punct removal

text_tokenized <- text_df %>%
  unnest_tokens(word,all_text)

text_tokenized

#Remove stopwords

data(stop_words)

text_tokenized<- text_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  #NOT SURE IF THIS LINE SHOULD REMAIN
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

# fix the script so it doesn't pick up these file names, numbers  
# forcibly removing for now


# Word Count

text_word_ct <- text_tokenized %>%
  count(word, sort=TRUE)
```


# Quanteda

```{r}
# install.packages("readtext")
# install.packages("quanteda")
# install.packages("quanteda.textstats") 
#https://rdrr.io/github/quanteda/quanteda.textstat/f/README.md
library(quanteda)
library(quanteda.textstats)
library(readtext)
lynch1 <- readtext("../test_class/articles_cleaned_2023_03_08")
```

### creates index with metadata
```{r}
###
# List out text files that match pattern .txt, create DF
###
# files <- list.files("../sample_corpus_1pct_article_text/", pattern="*.txt")
# # files <- list.files("../article_text_7_15/article_text/", pattern="*.txt") %>% 
#   as.data.frame() %>%
#   rename(filename = 1) %>%
#   filter(!str_detect(filename,"log"))
# 
# 
# ###
# # Load 638 stories provided by jack, create join column, join to files list
# ###
# 
# jackindex <- read_csv("../article_text_7_15/article_text/LayoutBoxes_index.csv") %>%
#   mutate(filename = paste0(file_id,"_",article_id,".txt")) %>%
#   inner_join(files) %>%
#   mutate(filepath = paste0("../article_text_7_15/article_text/",filename))
```

### adds metadata to corpus
```{r}
lynch1 <- lynch1 %>% 
  inner_join(jackindex, by=c("doc_id"="filename"))

##below is the code to add decade and region metadata

#### Compile by decade
lynch2 <- lynch1 %>% 
  mutate(decade = case_when(
         year < 1870 ~ "pre1870",
        year >= 1870 & year <=1879 ~ "1870s",
         year >= 1880 & year <=1889 ~ "1880s",
         year >= 1890 & year <=1899 ~ "1890s",
        year >= 1900 & year <=1909 ~ "1900s",
        year >= 1910 & year <=1919 ~ "1910s",
        year >= 1920 & year <=1929 ~ "1920s",
        year >= 1930 ~ "post1930s"
         ))

#### Compile by region
lynch2 <- lynch2 %>% 
  mutate(region=newspaper_state) %>% 
  mutate(region = case_when(region=="South Carolina" ~ "South",
                            region=="Texas" ~ "South",
                            region=="Louisiana" ~ "South",
                            region=="Tennessee" ~ "South",
                            region=="Mississippi" ~ "South",
                            region=="Arkansas" ~ "South",
                            region=="Alabama" ~ "South",
                            region=="Georgia" ~ "South",
                            region=="Virginia" ~ "South",
                            region=="Florida" ~ "South",
                            region=="North Carolina" ~ "South",
                            region=="Maryland" ~ "Border",
                            region=="Delaware" ~ "Border",
                            region=="West Virginia" ~ "Border",
                            region=="Kentucky" ~ "Border",
                            region=="Missouri" ~ "Border",
                            region=="Maine" ~ "North",
                            region=="New York" ~ "North",
                            region=="New Hampshire" ~ "North",
                            region=="Vermont" ~ "North",
                            region=="Massassachusetts" ~ "North",
                            region=="Connecticut" ~ "North",
                            region=="Rhode Island" ~ "North",
                            region=="Pennsylvania" ~ "North",
                            region=="New Jersey" ~ "North",
                            region=="Ohio" ~ "North",
                            region=="Indiana" ~ "North",
                            region=="Kansas" ~ "North",
                            region=="Michigan" ~ "North",
                             region=="Wisconsin" ~ "North",
                             region=="Minnesota" ~ "North",
                             region=="Iowa" ~ "North",
                             region=="California" ~ "North",
                             region=="Nevada" ~ "North",
                             region=="Oregon" ~ "North",
                            region=="Illinois" ~ "North",
                            region=="Nebraska" ~ "Misc",
                            region=="Colorado" ~ "Misc",
                            region=="North Dakota" ~ "Misc",
                            region=="South Dakota" ~ "Misc",
                            region=="Montana" ~ "Misc",
                            region=="Washington" ~ "Misc",
                            region=="Idaho" ~ "Misc",
                            region=="Wyoming" ~ "Misc",
                            region=="Utah" ~ "Misc",
                            region=="Oklahoma" ~ "Misc",
                            region=="New Mexico" ~ "Misc",
                            region=="Arizona" ~ "Misc",
                            region=="Alaska" ~ "Misc",
                            region=="Hawaii" ~ "Misc",
                            region=="District of Columbia" ~ "Misc",
                            region=="Virgin Islands" ~ "Misc",
                                                     TRUE~region)) 



```


```{r}
#lower case
lynch2$text <- tolower(lynch2$text)

my_corpus <- corpus(lynch2)  # build a new corpus from the texts

```


### KWIC: 
#### Mob analysis
```{r}

mob_analysis <- kwic(my_corpus, phrase(c("mob", "masked men", "mask", "captors", "a party of", "mob", "mob attacked", "mob caught", "overpowered by a mob")), window = 50, valuetype = "regex") %>% as.data.frame() 

mob_analysis <- mob_analysis %>% 
  inner_join(jackindex, by=c("docname"="filename"))

#Eliminate duplication
mob_analysis <- mob_analysis %>% 
  distinct(docname, from, to, pre, keyword, post, sn, newspaper_name, year, month, day, URL, newspaper_state)

write_csv(mob_analysis, "../J389L/mob_analysis_april28.csv")

```

```{r}
#Code: hostile= 1; orderly= 2; neutral =3; unclear= 4, irrelevant =5; duplicate = 0
mob <- rio::import("https://docs.google.com/spreadsheets/d/1MChohD5Oy9f_71nWCx8srMh8CcktoYGF8-bGfB0uOV8/edit#gid=1436302326")

mob <- mob %>% 
  rename(code = 'Code: hostile= 1; orderly= 2; neutral =3; unclear= 4, irrelevant =5; duplicate = 0')

mob$decade <- paste0(substr(mob$year, 0, 3), "0")

#exclude 0, 4, 5: unclear= 4, irrelevant =5; duplicate = 0

mob_sum <- mob %>% 
  group_by(code, decade) %>% 
  filter(!(code %in% c("0", "4", "5")))%>%
  filter(!is.na(code)) %>% 
  count(code) %>% 
   mutate(
     category= case_when(
        code == '1'~ "hostile",
        code == '2' ~ "orderly",
        code == '3'~ "neutral"
      )
  ) 

#normalize
mob_sum <- mob_sum %>% 
  select(code, decade, n, category) %>%  
  group_by(decade) %>% 
  mutate(decade_total = (n/sum(n))) %>% 
  ungroup()
            
    

```
Visualize
```{r}
mob_sum %>% 
ggplot(aes(x = decade, y = decade_total, fill = category)) +
  geom_col(position = "dodge") + 
    labs(title = "Mob Disposition", 
       subtitle = "Content Analysis of Mob Narrative. Pct of Category / Decade",
       caption = "Graphic by Rob Wells & Mohamed Salama, 4/9/2023",
       y="Pct of Category",
       x="Year")
```



Visualize
```{r}
mob_sum %>% 
ggplot(aes(x = decade, y = n, fill = category)) +
  geom_col(position = "dodge") + 
    labs(title = "Mob Disposition", 
       subtitle = "Content Analysis of Mob Narrative",
       caption = "Graphic by Rob Wells & Mohamed Salama, 4/9/2023",
       y="Count of Category",
       x="Year")



```

#### Sheriff analysis

```{r}
#quanteda - can add words before / after with window = X
#https://quanteda.io/reference/kwic.html
sheriff_analysis <- kwic(my_corpus, "sheriff", window = 20, valuetype = "regex") %>% as.data.frame() 

sheriff_analysis <- sheriff_analysis %>% 
  inner_join(jackindex, by=c("docname"="filename"))

# write_delim(sheriff_analysis, path = "sheriff_analysis.txt", delim = ";")
# write_csv(sheriff_analysis, "../J389L/sheriff_analysis_april28.csv")
```

#### Lynch Law analysis

```{r}
lynchlaw_analysis <- kwic(my_corpus, phrase(c("lynch law")), window = 20, valuetype = "regex") %>% as.data.frame() 

lynchlaw_analysis <- lynchlaw_analysis %>% 
  inner_join(jackindex, by=c("docname"="filename"))

write_csv(lynchlaw_analysis, "../J389L/lynchlaw_analysis_april28.csv")
```
#### Women

```{r}
#quanteda - can add words before / after with window = X
#https://quanteda.io/reference/kwic.html
woman_analysis <- kwic(my_corpus,  phrase(c("woman", "girl", "wife", "daughter", "lady")), window = 20, valuetype = "regex") %>% as.data.frame() 

woman_analysis <- woman_analysis %>% 
  inner_join(jackindex, by=c("docname"="filename"))

write_csv(woman_analysis,  "../J389L/woman_analysis_april28.csv")
```

#### Lynch Victim

```{r}
lynchvictim_analysis <- kwic(my_corpus,  phrase(c("negro", "fiend", "murderer")), window = 20, valuetype = "regex") %>% as.data.frame() 

lynchvictim_analysis <- lynchvictim_analysis %>% 
  inner_join(jackindex, by=c("docname"="filename"))

write_csv(lynchvictim_analysis,  "../J389L/lynchvictim_analysis_april28.csv")
```


####Citizen analysis

```{r}
#quanteda - can add words before / after with window = X
#https://quanteda.io/reference/kwic.html
citizen_analysis <- kwic(my_corpus, "citizen", window = 10, valuetype = "regex") %>% as.data.frame() 

citizen_analysis <- citizen_analysis %>% 
  inner_join(jackindex, by=c("docname"="filename"))


citizen_analysis %>% 
  count(docname) %>% 
  arrange(desc(n))

citizen1 <- citizen_analysis %>% 
  select(docname, pre, keyword, post, year) %>% 
  mutate(phrase = paste(pre,keyword,post, sep = ' ')) %>% 
  select(docname, year, phrase)

#write_csv(citizen1, here::here("../hcij_lynching_phase_two/narratives/data/citizen_3_8.csv"))
```

### Phrase Searching
```{r}
#Phrase searching
#"mob law"
#"hanged by", "lynched by", "strung up", "shot by", "burned by"
#https://quanteda.io/reference/phrase.html
#https://quanteda.io/reference/pattern.html
hanged_by <- kwic(my_corpus, phrase(c("hanged by", "lynched by", "strung up", "shot by", "burned by", "swung")), window = 15, valuetype = "regex") %>% as.data.frame() 

hanged_by <- hanged_by %>% 
  inner_join(jackindex, by=c("docname"="filename"))

#write_csv(hanged_by, "hanged_by_jan_12.csv")

#lynchers to justice 
# lynchers_to_justice <- kwic(my_corpus, phrase(c("lynchers to justice")), window = 15, valuetype = "regex") %>% as.data.frame() 
```

#### Extreme violence 
Terms: "roasted","riddled", "corpse","burned at the stake", "corpse"
```{r}
violence <- kwic(my_corpus, phrase(c("roasted", "riddled", "corpse", "burned at the stake")), window = 15, valuetype = "regex") %>% as.data.frame() 

violence <- violence %>% 
  inner_join(jackindex, by=c("docname"="filename"))
  
violence <- violence %>% 
  filter(!docname==("13733_0.txt"))

```


```{r}
#Summarize the graphic violence, one article only (even if multiple examples inthe article), by year
x <- violence %>% 
  select(docname, year) %>% 
  group_by(docname, year) %>% 
  count(year) %>% 
  ungroup() %>% 
  count(year) %>% 
  arrange(desc(n))

```


```{r}

violence %>% 
  select(docname, year) %>% 
  group_by(year) %>% 
  count(year) %>% 
ggplot(aes(x = year, y = n, fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "Extreme Violence By Year, 1789-1963", 
       subtitle = "Count of News Pages with Lynching Stories",
       caption = ". Graphic by Rob Wells, 2/28/2023",
       y="Count of Pages",
       x="Year")

#write_csv(hanged_by, "hanged_by_jan_12.csv")

```

#### Racist Language
Terms: "nigger","fiend", "brute" 
"savage" and "coon" not used because they crop up as variants and can describe the mob
```{r}
racist <- kwic(my_corpus, phrase(c("nigger","fiend", "brute")), window = 15, valuetype = "regex") %>% as.data.frame() 

racist <- racist %>% 
  inner_join(jackindex, by=c("docname"="filename"))
  
racist <- racist %>% 
  filter(!docname==("13733_0.txt"))

junk <- c("fiendish", "fiendishly", "fiends")
racist <- racist %>% 
  filter(!keyword %in% junk)

```

```{r}
#Summarize the racist language
racist %>%
  count(newspaper_name) %>% 
  arrange(desc(n))

#Newspapers with explicit racist language in sample
# The broad ax (Salt Lake City,...)	11
# Santa Fe new Mexican (Santa Fe, N.M.)	5
# The gold leaf (Henderson, N.C.)	3
# Abilene weekly reflector (Abilene, Kan.)	2
# Bismarck daily tribune (Bismarck, Dakot...)	2
# Chicago daily tribune (Chicago, Ill.) 1872-1963	2
# Evening journal (Wilmington, Del...)	2
# Los Angeles herald (Los Angeles [Calif.]) 1890-1893	2
# The Bridgeport evening farmer (Bridgeport, Con...)	2


```

```{r}

racist %>%
  select(newspaper_name, year) %>% 
  count(year) %>% 
ggplot(aes(x = year, y = n, fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "Racist Language By Year", 
       subtitle = "Count of News Pages with Lynching Stories",
       caption = ". Graphic by Rob Wells, 3/2/2023",
       y="Count of Pages",
       x="Year")

#write_csv(hanged_by, "hanged_by_jan_12.csv")

```




#### Grand jury
```{r}
#grand jury 
grand_jury <- kwic(my_corpus, phrase(c("grand jury")), window = 20, valuetype = "regex") %>% as.data.frame() 


grand_jury <- grand_jury  %>% 
  inner_join(jackindex, by=c("docname"="filename"))

#write_csv(grand_jury, "grand_jury_jan_12.csv")
```

#### Grand jury analysis
```{r}

#excel coded, reimporting
grand_jury <- rio::import("grand_jury_jan_12.csv")

grand_jury %>% group_by(Code) %>% 
  filter(!Code=="") %>% 
  summarise(count=n())   %>% 
  distinct() %>% 
  arrange(desc(count)) 

```

#### Hanged Analysis
```{r}
hanged_by1 <- rio::import("hanged_by_jan_12.csv")

hanged_by1 %>% group_by(Mob_violence_Y_N, `Citizen_violence_Y-N`) %>% summarise(count=n())

#An examination of the context of the terms "hanged by", "lynched by", "strung up", "shot by", "burned by" showed 67 articles describing mob violence and nine describing "citizens" somehow involved in a lynching.

```


#### Interview analysis

```{r}
#interview
interview <- kwic(my_corpus, phrase(c("interview", "according to", "negro said", "stated that", "speaking", "incident", "told by", "statement", "when questioned", "when asked")), window = 40, valuetype = "regex") %>% as.data.frame() 


interview  <- interview   %>% 
  inner_join(jackindex, by=c("docname"="filename"))

#no results from:
#negro talk

write_csv(interview , "interview_jan_20.csv")
```
#### Coded Interview analysis
The interview Jan 20 was coded for these types of sources: Black, Confession, Government_Official, Law, Lynchers, Lynching_Victim, News_Media, Not_Relevant, Other, Repeats_Earlier, Unclear, Victim_of_Crime, Witness
```{r}
coded_interview <- rio::import("coded_interview_jan_20.csv")

#94 articles
# coded_interview %>% 
#   select(file_id) %>% 
#   group_by(file_id) %>%
#   distinct() 

sourcing <- coded_interview %>% 
  group_by(Code) %>% 
  summarise(count=n()) %>% 
  mutate(pct_total =round(count/sum(count), digits=2)) %>% 
  arrange(desc(count))

write.csv(sourcing, "sourcing_jan20.csv")
```

#### Lynching - Analysis of the act

    Lynching: Portrayal / Description 
    Level of graphic detail
    Illustrations
    Was it a single lynching or a race massacre
    Was it called a crime 
    Was it called social disorder, reflects badly on community
    Was it called justice
    
    keywords for lynching:
    rope bleeding half-dead shouts of glee tree posse  swingin strung hang fiend  negro's body mob attacked mob caught mob overpowered by a mob was taken from an angry mob 


```{r}
#Lynching - Analysis of the act
lynching_act <- kwic(my_corpus, phrase(c("rope", "bleeding", "shouts", "tree", "posse", "swing", "strung", "hang", "fiend", "body")), window = 40, valuetype = "regex") %>% as.data.frame() 


lynching_act <- lynching_act  %>% 
  inner_join(jackindex, by=c("docname"="filename"))

#no results from:


write_csv(lynching_act , "lynching_act_jan_25.csv")

lynching_act %>% 
  count(keyword) %>% 
  arrange(desc(n))

```







# Word and text similarity and distance
https://content-analysis-with-r.com/2-metrics.html

```{r}
#code used to analyze "sheriff" "mob" "negro"
corpus.sentences <- corpus_reshape(my_corpus, to = "sentences")
dfm.sentences <- dfm(corpus.sentences, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove = stopwords("english"))
dfm.sentences <- dfm_trim(dfm.sentences, min_docfreq = 5)

similarity.words <- textstat_simil(dfm.sentences, dfm.sentences[,"citizen"], margin = "features", method = "simple matching")

x <- similarity.words %>% 
  as.data.frame() %>% 
  janitor::clean_names() %>% 
  arrange(desc(simple_matching))

#write.csv(x, "citizen_words.csv")
```

```{r}
head(similarity.words[order(similarity.words[,1], decreasing = T),], 10)


```
#### Simple matching Graphic violence
dictionary <- data.frame(word=c("roasted","riddled", "corpse","burned","cut","corpse"))

```{r}
dictionary <- data.frame(word=c("roasted","riddled", "corpse","burned","cut","corpse"))

corpus.sentences <- corpus_reshape(my_corpus, to = "sentences")
dfm.sentences <- dfm(corpus.sentences, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove = stopwords("english"))
dfm.sentences <- dfm_trim(dfm.sentences, min_docfreq = 5)
similarity.words <- textstat_simil(dfm.sentences, dfm.sentences[,"roasted"], margin = "features", method = "simple matching")


corpus.sentences <- corpus_reshape(my_corpus, to = "sentences")
dfm.sentences <- dfm(corpus.sentences, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove = stopwords("english"))
dfm.sentences <- dfm_trim(dfm.sentences, min_docfreq = 5)
similarity.words <- textstat_simil(dfm.sentences, dfm.sentences[,dictionary], margin = "features", method = "simple matching")

x <- similarity.words %>% 
  as.data.frame() %>% 
  janitor::clean_names() %>% 
  arrange(desc(simple_matching)) 

# write.csv(x, "citizen_words.csv")
```
```{r}
#13733_0.txt is an weird outlier


misc <- readtext("/Users/robwells/Code/hcij_lynching_phase_two/article_text_2023-02-01/13733_0.txt")


```



# NOTES: TOPIC MODELING WITH LADA
FROM https://ladal.edu.au/topicmodels.html

```{r}
# # install packages
# install.packages("tm")
# install.packages("topicmodels")
# install.packages("reshape2")
# #install.packages("ggplot2")
# install.packages("wordcloud")
# install.packages("pals")
# install.packages("SnowballC")
# install.packages("lda")
# install.packages("ldatuning")
# install.packages("kableExtra")
# install.packages("DT")
# install.packages("flextable")
# # install klippy for copy-to-clipboard button in code chunks
# install.packages("remotes")
# remotes::install_github("rlesur/klippy")



```

```{r}
# set options
options(stringsAsFactors = F)         # no automatic data transformation
options("scipen" = 100, "digits" = 4) # suppress math annotation
# load packages
library(knitr) 
library(kableExtra) 
library(DT)
library(tm)
library(topicmodels)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(SnowballC)
library(lda)
library(ldatuning)
library(flextable)
# activate klippy for copy-to-clipboard button
klippy::klippy()


```
```{r}
# load data
# textdata <- base::readRDS(url("https://slcladal.github.io/data/sotu_paragraphs.rda", "rb"))

textdata <- lynch1 %>% 
  as.data.frame()

# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")
# create corpus object
corpus <- Corpus(DataframeSource(textdata))
# Preprocessing chain
processedCorpus <- tm_map(corpus, content_transformer(tolower))
processedCorpus <- tm_map(processedCorpus, removeWords, english_stopwords)
processedCorpus <- tm_map(processedCorpus, removePunctuation, preserve_intra_word_dashes = TRUE)
processedCorpus <- tm_map(processedCorpus, removeNumbers)
processedCorpus <- tm_map(processedCorpus, stemDocument, language = "en")
processedCorpus <- tm_map(processedCorpus, stripWhitespace)



```

```{r}
# compute document term matrix with terms >= minimumFrequency
minimumFrequency <- 5
DTM <- DocumentTermMatrix(processedCorpus, control = list(bounds = list(global = c(minimumFrequency, Inf))))
# have a look at the number of documents and terms in the matrix
dim(DTM)
#[1] 1465 4291

```

```{r}
# due to vocabulary pruning, we have empty rows in our DTM
# LDA does not like this. So we remove those docs from the
# DTM and the metadata
sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]
textdata <- textdata[sel_idx, ]

```

```{r}
# create models with different number of topics
result <- ldatuning::FindTopicsNumber(
  DTM,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)


```
```{r}
FindTopicsNumber_plot(result)

```


```{r}
# number of topics
K <- 20
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))


```

```{r}
# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)


```
```{r}
# topics are probability distributions over the entire vocabulary
beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms

```

```{r}
# for every document we have a probability distribution of its contained topics
theta <- tmResult$topics 
dim(theta)               # nDocs(DTM) distributions over K topics

```

```{r}
terms(topicModel, 10)
```


```{r}
exampleTermData <- terms(topicModel, 10)
exampleTermData[, 1:8]

```

```{r}
exampleIds <- c(2, 100, 200)
lapply(corpus[exampleIds], as.character)
```


Topic Ranking
```{r}
# re-rank top topic terms for topic names
topicNames <- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = " ")


```

```{r}
# What are the most probable topics in the entire collection?
topicProportions <- colSums(theta) / nDocs(DTM)  # mean probabilities over all paragraphs
names(topicProportions) <- topicNames     # assign the topic names we created before
sort(topicProportions, decreasing = TRUE) # show summed proportions in decreased order


```

```{r}
soP <- sort(topicProportions, decreasing = TRUE)
paste(round(soP, 5), ":", names(soP))


```

Counts of topic
```{r}
countsOfPrimaryTopics <- rep(0, K)
names(countsOfPrimaryTopics) <- topicNames
for (i in 1:nDocs(DTM)) {
  topicsPerDoc <- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}
sort(countsOfPrimaryTopics, decreasing = TRUE)


```

```{r}
so <- sort(countsOfPrimaryTopics, decreasing = TRUE)
paste(so, ":", names(so))

so <- so %>% 
  as.data.frame()


```

```{r}
topicModel2 <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25, alpha = 0.2))
tmResult <- posterior(topicModel2)
theta <- tmResult$topics
beta <- tmResult$terms
topicNames <- apply(terms(topicModel2, 5), 2, paste, collapse = " ")  # reset topicnames
top5termsPerTopic <- terms(topicModel, 5)
topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")


```

```{r}
# append decade information for aggregation
textdata$decade <- paste0(substr(textdata$year, 0, 3), "0")

# textdata <- textdata %>% 
#   mutate(decade = case_when(
#          year < 1870 ~ "pre1870",
#         year >= 1870 & year <=1879 ~ "1870s",
#          year >= 1880 & year <=1889 ~ "1880s",
#          year >= 1890 & year <=1899 ~ "1890s",
#         year >= 1900 & year <=1909 ~ "1900s",
#         year >= 1910 & year <=1919 ~ "1910s",
#         year >= 1920 & year <=1929 ~ "1920s",
#         year >= 1930 ~ "post1930s"
#          ))

```


```{r}
# get mean topic proportions per decade
topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$decade), mean)
# set topic names to aggregated columns
colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_decade, id.vars = "decade")
# plot topic proportions per decade as bar plot
ggplot(vizDataFrame, aes(x=decade, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = paste0(alphabet(20), "FF"), name = "decade") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```


# NOTES: Words Within Proximity

https://stackoverflow.com/questions/58187923/find-documents-with-two-words-in-a-given-proximity-in-r

transcripts <- data.frame(text=c("Republicans in congress today voted on a bill","Republicans today passed a bill to allocate funds for Congress")
dictionary <- data.frame(word=c("Congress","Capitol"))

transcripts_subset <- transcripts %>%
  filter(grepl(paste(dictionary$word, collapse="|"), text))
  
dictionary <- data.frame(word=c("Congress","Capitol"), stringsAsFactors = FALSE)

pattern_after  <- paste0("\\b(", paste0(dictionary$word, collapse="|"), ")\\W+(?:\\w+\\W+){0,5}?(Republican(s)*|Democrat(s)*)")
pattern_before <- paste0("\\b(Republican(s)*|Democrat(s)*)\\W+(?:\\w+\\W+){0,5}?", paste0(dictionary$word, collapse="|"), collapse="|")
pattern <- paste0(c(pattern_after, pattern_before), collapse="|")
pattern
#> [1] "\\b(Congress|Capitol)\\W+(?:\\w+\\W+){0,5}?(Republican(s)*|Democrat(s)*)|\\b(Republican(s)*|Democrat(s)*)\\W+(?:\\w+\\W+){0,5}?Congress|Capitol"


grepl(pattern, "Republicans in congress today voted on a bill", perl = TRUE, ignore.case = TRUE)
#> [1] TRUE

grepl(pattern, "Democrats today passed a bill to allocate funds for Congress", perl = TRUE, ignore.case = TRUE)
#> [1] FALSE

grepl(pattern, "A Democrat in Congress", perl = TRUE, ignore.case = TRUE)
#> [1] TRUE


```{r}
transcripts <- lynch1 %>% 
  as.data.frame()

dictionary <- data.frame(word=c("roasted","riddled", "corpse","burned","cut","corpse"))

transcripts_subset <- transcripts %>%
  filter(grepl(paste(dictionary$word, collapse="|"), text))

pattern_after  <- paste0("\\b(", paste0(dictionary$word, collapse="|"), ")\\W+(?:\\w+\\W+){0,10}?(negro*)")
pattern_before <- paste0("\\b(negro*)\\W+(?:\\w+\\W+){0,10}?", paste0(dictionary$word, collapse="|"), collapse="|")
pattern <- paste0(c(pattern_after, pattern_before), collapse="|")
pattern

x <- grepl(pattern, transcripts_subset, perl = TRUE, ignore.case = TRUE)

```



### subset corpus

```{r}

#xpre1880 <- summary(corpus_subset(my_corpus, year < 1880))


# border_corpus <- lynch2 %>% 
#   filter(region=="Border") %>% 
#   corpus()

# south_corpus <- lynch2 %>% 
#   filter(region=="South") %>% 
#   corpus()

# north_corpus <- lynch2 %>% 
#   filter(region=="North") %>% 
#   corpus()

misc_corpus <- lynch2 %>% 
  filter(region=="Misc") %>% 
  corpus()
```

### Topic Modeling- Subsets
```{r}
#change variable
my_corpus1 <- tokens(misc_corpus, remove_numbers = TRUE,  remove_punct = TRUE)

quant_dfm <- dfm(my_corpus1, 
                remove_punct = TRUE, remove_numbers = TRUE, remove = stopwords("english"))

#dfm_remove = stopwords("english"))
# quant_dfm <- dfm_trim(quant_dfm, min_termfreq = 4, max_docfreq = 10)
quant_dfm

topfeatures(quant_dfm, 50)

```

```{r}
#install.packages("stm")
set.seed(100)
if (require(stm)) {
    my_lda_fit20 <- stm(quant_dfm, K = 20, verbose = FALSE)
    plot(my_lda_fit20)    
}

```

### Topic Modeling - Whole Corpus
```{r}

my_corpus1 <- tokens(my_corpus, remove_numbers = TRUE,  remove_punct = TRUE)

#removes stopwords
# quant_dfm <- dfm(my_corpus1, 
#                 remove_punct = TRUE, remove_numbers = TRUE, remove = stopwords("english"))

#keeps stopwords - needed for attribution
quant_dfm <- dfm(my_corpus1, 
                remove_punct = TRUE, remove_numbers = TRUE)

#dfm_remove = stopwords("english"))
# quant_dfm <- dfm_trim(quant_dfm, min_termfreq = 4, max_docfreq = 10)
quant_dfm

topfeatures(quant_dfm, 50)

```


## Keyword In Context Analysis 

```{r}
#said
#interview



said <- kwic(my_corpus, "said", valuetype = "regex") %>% as.data.frame()
# write.csv(said, "attribution_said.csv")
# write.csv(interviews, "attribution_interview.csv")

quanteda_test <- kwic(my_corpus, "watts", valuetype = "regex") %>% as.data.frame()

  
#write.csv(quanteda_test, "quanteda_test.csv")

```


```{r}
#install.packages("stm")
set.seed(100)
if (require(stm)) {
    my_lda_fit20 <- stm(quant_dfm, K = 20, verbose = FALSE)
    plot(my_lda_fit20)    
}

```


# Document Term Matrix from a Corpus
https://sicss.io/2020/materials/day3-text-analysis/basic-text-analysis/rmarkdown/Basic_Text_Analysis_in_R.html#the-document-term-matrix

```{r}
library(tidytext)
#install.packages("tm")
library(tm)
lynch_DTM <- DocumentTermMatrix(my_corpus, control = list(wordLengths = c(2, Inf)))
```


```{r}
inspect(lynch_DTM[1:5, 3:8])

```

```{r}
tidy_lynch<- lynch2 %>%
    select(doc_id,text) %>%
    unnest_tokens("word", text)
head(tidy_lynch)



tidy_lynch_DTM<-
  tidy_lynch %>%
  count(doc_id, word) %>%
  cast_dtm(doc_id, word, n)

inspect(tidy_lynch_DTM[1:20,40:50])
```


# Search Phrases

Count articles with 'sheriff said'  - try to get it "sheriff" (within 5 words) "said"
https://stackoverflow.com/questions/68214543/search-for-word-phrase-from-column-in-r
```{r}
x <- lynch2 %>% 
  mutate(A = +str_detect(text,str_c(c('sheriff said'), collapse = '|')),
     B = +str_detect(text,str_c(c('sheriff warned'), collapse = '|')),
     C= +str_detect(text, str_c(c(pattern = "\\sheriff$"), collapse = '|')))
```


```{r}
checkwords=lapply(lynch1$text,
FUN=function(str,words=c("sheriff","said"))
{
  sapply(strsplit(str,"\\.")[[1]],FUN=function(el){
    any(all(sapply(words,
           FUN=function(wd)grepl(wd,el))))
     })
})
checkwords

grepl("sheriff[^\\.,!?:;]*said", lynch1$text)



```


https://www.regular-expressions.info/near.html
```{r}

grepl('\b(?:"sheriff"\W+(?:\w+\W+){1,6}?"said"|"said"\W+(?:\w+\W+){1,6}?"sheriff")\b', lynch1$text)


```


Quanteda Notes
https://quanteda.io/articles/pkgdown/quickstart.html
Similarities between texts

pres_dfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980), 
               remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
obama_simil <- textstat_simil(pres_dfm, pres_dfm[c("2009-Obama" , "2013-Obama"),], 
                             margin = "documents", method = "cosine")
obama_simil


Topic models

# #-------------#
# Appendix
# #-------------#

### Sean's compiler of raw text. 
```{r include=FALSE}

#This was run once to compile the lynch object and "articles_1pct_dec26.csv" and then used for tokenizing, etc

#####################
# Begin SM Code #####
#####################

###
# List out text files that match pattern .txt, create DF
###

files <- list.files("../sample_corpus_1pct_article_text/", pattern="*.txt") %>% 
  as.data.frame() %>%
  rename(filename = 1) %>%
  filter(!str_detect(filename,"_bak"))


###
# Load 848 stories provided by jack, create join column, join to files list
###

jackindex <- read_csv("../sample_corpus_1pct_article_text/stratified_sample_manifest.csv") %>%
  mutate(filename = paste0(file_id,"_",article_id,".txt")) %>%
  inner_join(files) %>%
  mutate(filepath = paste0("../sample_corpus_1pct_article_text/",filename))

#The data is here
#https://github.com/Howard-Center-Investigations/hcij_lynching_phase_two/blob/main/article_text_7_15/article_text_2022-07-14.tar.gz

###
# Define function to loop through each text file referenced in jackindex df
###

create_article_text <- function(row_value) {
  
  #row_value is the single argument that is passed to the function
  # Take each row of the dataframe
  temp <- jackindex %>%
    slice(row_value)
  
  # Store the filename for  use in constructing articles dataframe
  temp_filename <- temp$filename
  
  # Create a dataframe by reading in lines of a given textfile
  # Add a filename column 
  articles_df_temp <- read_lines(temp$filepath) %>%
    as_tibble() %>%
    mutate(filename = temp_filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
  articles_df <<- articles_df %>%
    bind_rows(articles_df_temp)
}

###
# Create elements needed to run function
###

# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2) 
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe 
row_values <- 1:nrow(jackindex)

###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###

lapply(row_values, create_article_text)

###
# Clean up articles_df and join to index dataframe
###

articles_df <- articles_df %>%
  select(filename, sentence=value) %>%
  inner_join(jackindex)


#####################
# End SM Code #####
#####################

#write.csv(articles_df, "articles_1pct_dec26.csv")
#lynch <- articles_df
#write_csv(jackindex, "jackindex_dec26.csv")
```

