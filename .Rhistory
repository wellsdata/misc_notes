inner_join(pop_hisp, by="community")
# Output dataframes
max_transit_race
min_transit_race
View(max_transit_race)
low_public_high_drive <- min_public_transportation %>%
clean_names() %>%
inner_join(no_car, by="community") %>%
arrange(desc(x2020_data))
high_public_low_drive <- max_public_transportation %>%
clean_names() %>%
inner_join(drive_to_work, by="community") %>%
arrange(desc(x2020_data))
library(tidyverse)
library(tidycensus)
library(rvest)
library(janitor)
library(leaflet)
library(tigris)
library(ggplot2)
census_api_key("d1cec91598a591a7005eebc637decda2109a24de", overwrite =TRUE)
#Since it appears that ACS data can only be imported one year at a tme, we created separate datasets for each year from 2010-2020 for median ##SPELLING househol ## income. Then, we combined the datasets into one, showing the median income in each Maryland county from 2010 to 2020
wealth_2020 <- get_acs(geography = "county",
variables = c(median_income = "B19013_001"),
state = "MD",
year = 2020) %>%
clean_names() %>%
rename(estimate_2020 = estimate) %>%
select(name, estimate_2020, geoid)
wealth_2019 <- get_acs(geography = "county",
variables = c(median_income = "B19013_001"),
state = "MD",
year = 2019) %>%
clean_names() %>%
rename(estimate_2019 = estimate) %>%
select(name, estimate_2019, geoid)
wealth_2018 <- get_acs(geography = "county",
variables = c(median_income = "B19013_001"),
state = "MD",
year = 2018) %>%
clean_names() %>%
rename(estimate_2018 = estimate) %>%
select(name, estimate_2018, geoid)
wealth_2017 <- get_acs(geography = "county",
variables = c(median_income = "B19013_001"),
state = "MD",
year = 2017) %>%
clean_names() %>%
rename(estimate_2017 = estimate) %>%
select(name, estimate_2017, geoid)
wealth_2016 <- get_acs(geography = "county",
variables = c(median_income = "B19013_001"),
state = "MD",
year = 2016) %>%
clean_names() %>%
rename(estimate_2016 = estimate) %>%
select(name, estimate_2016, geoid)
wealth_2015 <- get_acs(geography = "county",
variables = c(median_income = "B19013_001"),
state = "MD",
year = 2015) %>%
clean_names() %>%
rename(estimate_2015 = estimate) %>%
select(name, estimate_2015, geoid)
wealth_2014 <- get_acs(geography = "county",
variables = c(median_income = "B19013_001"),
state = "MD",
year = 2014) %>%
clean_names() %>%
rename(estimate_2014 = estimate) %>%
select(name, estimate_2014, geoid)
wealth_2013 <- get_acs(geography = "county",
variables = c(median_income = "B19013_001"),
state = "MD",
year = 2013) %>%
clean_names() %>%
rename(estimate_2013 = estimate) %>%
select(name, estimate_2013, geoid)
wealth_2012 <- get_acs(geography = "county",
variables = c(median_income = "B19013_001"),
state = "MD",
year = 2012) %>%
clean_names() %>%
rename(estimate_2012 = estimate) %>%
select(name, estimate_2012, geoid)
wealth_2011 <- get_acs(geography = "county",
variables = c(median_income = "B19013_001"),
state = "MD",
year = 2011) %>%
clean_names() %>%
rename(estimate_2011 = estimate) %>%
select(name, estimate_2011, geoid)
wealth_2010 <- get_acs(geography = "county",
variables = c(median_income = "B19013_001"),
state = "MD",
year = 2010) %>%
clean_names() %>%
rename(estimate_2010 = estimate) %>%
select(name, estimate_2010, geoid)
wealth_2010_2020 <- wealth_2010 %>%
inner_join(wealth_2011, by = "name") %>%
inner_join(wealth_2012, by = "name") %>%
inner_join(wealth_2013, by = "name") %>%
inner_join(wealth_2014, by = "name") %>%
inner_join(wealth_2015, by = "name") %>%
inner_join(wealth_2016, by = "name") %>%
inner_join(wealth_2017, by = "name") %>%
inner_join(wealth_2018, by = "name") %>%
inner_join(wealth_2019, by = "name") %>%
inner_join(wealth_2020, by = "name") %>%
select(name, geoid, estimate_2010, estimate_2011, estimate_2012, estimate_2013, estimate_2014, estimate_2015, estimate_2016, estimate_2017, estimate_2018, estimate_2019, estimate_2020)
#This final dataset shows the ACS estimate of median income in all Maryland counties from 2010 to 2020
View(wealth_2010_2020)
maryland_county_boundaries <- counties(state = 24, cb = FALSE, year = 2020) %>%
clean_names()
#I used the Tigris package, which is based on Census data and has shapefiles for all counties
county_wealth_2020 <- maryland_county_boundaries %>%
inner_join(wealth_2020, by = "geoid") %>%
select(geoid, namelsad, estimate_2020, geometry) %>%
rename(name = namelsad)
#This created a new table so that the 2020 median incomes were in the same rows as the shape of each county.
ggplot() +
geom_sf(data=maryland_county_boundaries) +
geom_sf(data=county_wealth_2020, aes(fill=estimate_2020))
library(tidyverse)
#install.packages("sampler")
library(sampler)
#install.packages("rio")
library(rio)
#install.packages("kableExtra")
library(kableExtra)
library(knitr)
rsampcalc(77996, e=3, ci=95,p=0.5, over=0)
#sample code: rsampcalc(N, e, ci=95,p=0.5, over=0)
#https://cran.r-project.org/web/packages/sampler/readme/README.html
rsampcalc(N=77996, e=3, ci=95,p=0.5, over=0)
index <- read_csv("index_oct7.csv") %>%
as.data.frame()
index <- janitor::clean_names(index)
sample2 <- rsamp(index, 1560, over=0, rep=FALSE)
View(sample2)
77995*.01
#2% of corpus:
77995*.02
#3% of corpus:
77995*.03
#5% of corpus:
77995*.05
sample_5pct <- rsamp(index, 3900, over=0, rep=FALSE)
View(sample_5pct)
sample1_count <- sample1 %>%
count(year) %>%
group_by(year) %>%
mutate(pct = (n/1053)*100)
sample1 <- rsamp(index, 1053, over=0, rep=FALSE)
sample1_count <- sample1 %>%
count(year) %>%
group_by(year) %>%
mutate(pct = (n/1053)*100)
sample1_count
sample_5pct_count <- sample_5pct %>%
count(year) %>%
group_by(year) %>%
mutate(pct = (n/3900)*100)
sample_5pct_count
View(sample_5pct_count)
#sample code: ssampcalc(df, n, strata, over=0)
x <- ssampcalc(index, n=77996, strata=year, over=0.5)
x <- janitor::clean_names(x)
x
#Fact check
#sum(x$nh)
#nh is the total pages per year
#wt[,1] is the percentage of the total corpus (77,996 pages)
#Shows peak media coverage of lynching activity between 1893-1910
# Draw stratified sample (proportional allocation)
# ssamp(df, n, strata, over=1)
#y <- ssamp(index, n=1053, strata=year, over=1)
y <- ssamp(index, n=3900, strata=year, over=1)
y
#fact check
#this shows a very close parallel to the stratified sample (x) done above of the 77,995 articles
y_pagecount <- y %>%
count(year) %>%
group_by(year) %>%
mutate(pct = (n/3900)*100)
#Write the sample corpus to a spreadsheet
write.csv(y, "sample_corpus_5pct.csv")
#1789-1963, 18.1 million rows
index_chron <- rio::import("/Volumes/GoogleDrive/My Drive/Research 2022/Lynching UMD/Data/Sampling and Full Chron America/chron_am_manifest.csv")
index_chron <- janitor::clean_names(index_chron)
#sample2 <- rsamp(index_chron, 1053, over=0, rep=FALSE)
# sample2_count <- sample1 %>%
# count(year) %>%
#    group_by(year) %>%
#   mutate(pct = (n/1053)*100)
sample3 <- rsamp(index_chron, 3900, over=0, rep=FALSE)
View(sample3)
sample3_count <- sample3 %>%
count(year) %>%
group_by(year) %>%
mutate(pct = (n/3900)*100)
sample3_count
View(sample3_count)
index_chron_year <- index_chron %>%
count(year) %>%
rename(pages =n)
sum2 <- sum(index_chron_year$pages)
index_chron_year <- index_chron_year %>%
mutate(pct_total_chron_pages = formattable::percent(pages/sum2, 2))
#sample code: rsampcalc(N, e, ci=95,p=0.5, over=0)
#https://cran.r-project.org/web/packages/sampler/readme/README.html
rsampcalc(18091200, e=3, ci=95,p=0.5, over=0)
library(tidyverse)
library(janitor)
library(rvest)
library(tidycensus)
census_api_key("549950d36c22ff16455fe196bbbd01d63cfbe6cf")
###
# Store url for governor results
###
gov_race_results <- "https://elections.maryland.gov/elections/2022/general_results/gen_results_2022_1.html"
###
# Read in results as html table
###
gov_race_results <- gov_race_results %>%
read_html() %>%
html_table()
###
# Extract dataframe, clean number columns, calculate pct on election day vote
###
gov_race_results <- gov_race_results[[1]] %>%
clean_names() %>%
mutate(across(c(3,4,5,7,8), parse_number)) %>%
mutate(pct_election_day = round(election_day/total*100,2)) %>%
slice(-7) %>%
arrange(pct_election_day) %>%
select(name, pct_election_day)
###
# Print it out
###
gov_race_results
gov_race_results <- gov_race_results %>%
read_html() %>%
html_table()
gov_race_results <- "https://elections.maryland.gov/elections/2022/general_results/gen_results_2022_1.html"
###
# Read in results as html table
###
gov_race_results <- gov_race_results %>%
read_html() %>%
html_table()
gov_race_results <- gov_race_results[[1]] %>%
clean_names()
View(gov_race_results)
gov_race_results <- gov_race_results[[1]] %>%
clean_names() %>%
mutate(across(c(3,4,5,7,8), parse_number)) %>%
mutate(pct_election_day = round(election_day/total*100,2)) %>%
slice(-7) %>%
arrange(pct_election_day) %>%
select(name, pct_election_day)
###
# Store url for governor results
###
gov_race_results <- "https://elections.maryland.gov/elections/2022/general_results/gen_results_2022_1.html"
###
# Read in results as html table
###
gov_race_results <- gov_race_results %>%
read_html() %>%
html_table()
###
# Extract dataframe, clean number columns, calculate pct on election day vote
###
gov_race_results <- gov_race_results[[1]] %>%
clean_names() %>%
mutate(across(c(3,4,5,7,8), parse_number)) %>%
mutate(pct_election_day = round(election_day/total*100,2)) %>%
slice(-7) %>%
arrange(pct_election_day) %>%
select(name, pct_election_day)
###
# Print it out
###
gov_race_results
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
Education_Stats <- read_csv("Education Stats - Copy of Digest 2018 Table 233.27.csv")
Education_stats_CLEAN<-Education_Stats %>%
clean_names()
Education_stats_CLEAN %>%
select(disability_status_sex_and_race_ethnicity, all_suspensions, all_expulsions) %>%
filter((disability_status_sex_and_race_ethnicity == "All_Students") | (disability_status_sex_and_race_ethnicity == "Disabled_students")) %>%
mutate(combined = all_suspensions + all_expulsions)
Education_stats_CLEAN %>%
select(disability_status_sex_and_race_ethnicity, all_suspensions, all_expulsions) %>%
filter((disability_status_sex_and_race_ethnicity == "All_Students") | (disability_status_sex_and_race_ethnicity == "Disabled_students")) %>%
mutate(Suspension_rate = all_suspensions/all_expulsions)
# create ratio of suspension and expulsion rate of disabled to all students
Education_stats_CLEAN %>%
select(disability_status_sex_and_race_ethnicity, all_suspensions, all_expulsions) %>%
filter((disability_status_sex_and_race_ethnicity == "All_White") |(disability_status_sex_and_race_ethnicity == "All_Black") | (disability_status_sex_and_race_ethnicity == "All_Hispanic") | (disability_status_sex_and_race_ethnicity == "All_Asian") | (disability_status_sex_and_race_ethnicity == "All_Pacific Islander") | (disability_status_sex_and_race_ethnicity == "All_American Indian/Alaska Native") | (disability_status_sex_and_race_ethnicity == "All_Two or more races")) %>%
mutate(discipline_by_race = all_suspensions + all_expulsions)
View(Education_Stats)
#install.packages("here")
here::here()
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
library(rio)
#install.packages("quanteda")
library(quanteda)
#import df created from Sean's compiler of raw text sequence - source code in appendix of this document
#formerly articles_1pct_dec26
lynch <- read_csv("../data/articles_march8.csv")
#index of 1 pct sample which has been checked by a coder and represents all valid entries
#formerly jackindex_dec26
jackindex <- read_csv("../data/jackindex_march8.csv")
#index of all 59,967 articles captured
# index <- read_csv("/Users/robwells/Code/hcij_lynching_phase_two/Storage_Older_Versions/index_oct7.csv") %>%
#   as.data.frame()
index <- read_csv("../data/index_feb6.csv") %>%
as.data.frame()
index <- janitor::clean_names(index)
#install.packages("here")
here::here()
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
library(rio)
#install.packages("quanteda")
library(quanteda)
#import df created from Sean's compiler of raw text sequence
black <- read_csv("../data/black_press_3_9_2023.csv")
#index of 714 black press from proquest and some from Howard
blackindex <- read_csv("../data/blackindex_3_9_2023.csv")
#index of the 59,967 articles from LOC
index <- read_csv("../data/index_jan20.csv") %>%
as.data.frame()
index <- read_csv("../data/index_feb6.csv") %>%
as.data.frame()
View(index)
index <- janitor::clean_names(index)
#243 papers from the 59,967 paper search
black_papers <- read_csv("../data/black_papers.csv") %>%
as.data.frame()
black_papers <- black_papers %>%
mutate(black = "Y")
View(black_papers)
black_years <- blackindex %>%
count(year) %>%
arrange(year)
View(black_years)
ggplot(black_years, aes(x=year, y=n, fill=n)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "Black Press Lynching Coverage in ProQuest",
subtitle= "Thin Coverage in late 19th Century During Peak Violence",
caption= "n=714 articles from ProQuest Historical Black Newspapers.
Graphic by Rob Wells",
x = "Year")
#ggsave("../output/proquest_black_lynchings_feb22.png",device = "png",width=9,height=6, dpi=800)
View(blackindex)
View(black_papers)
x <- jackindex %>%
left_join(black_papers, by=c("sn"="LCCN"))
View(index)
jackindex <- read_csv("../data/jackindex_march8.csv")
x <- jackindex %>%
left_join(black_papers, by=c("sn"="LCCN"))
x <- x %>%
select(file_id, article_id, sn, newspaper_name, year, month, day, page, edition, mod_id, URL, newspaper_state, filename,  filepath, black)
x %>%
count(black)
View(x)
blackpaperLCCN <- black_papers %>%
group_by(LCCN) %>%
count() %>%
arrange(desc(n))
View(black_papers)
View(blackpaperLCCN)
x <- black_papers %>%
filter(LCCN =="sn83016194" | LCCN == "sn83016810" | LCCN == "sn83016811" | LCCN ==  "sn84024055" | LCCN == "sn98058951") %>%
select(Title, State, LCCN, OCLC, `Persistent Link`)
View(x)
x <- jackindex %>%
left_join(black_papers, by=c("sn"="LCCN"))
x <- x %>%
select(file_id, article_id, sn, newspaper_name, year, month, day, page, edition, mod_id, URL, newspaper_state, filename,  filepath, black)
x %>%
count(black)
#76 black papers in our existing corpus
#1316 entries
#Annoying - index goes from 1465 to 1472 entries
View(x)
x %>%
count(file_id) %>%
arrange(desc(n))
zx <- x %>%
distinct(file_id)
View(zx)
zx <- x %>%
distinct()
View(zx)
zx %>%
count(file_id) %>%
arrange(desc(n))
x <- x %>%
select(file_id, article_id, sn, newspaper_name, year, month, day, page, edition, mod_id, URL, newspaper_state, filename,  filepath, black) %>%
filter(black=="Y")
View(x)
View(black_papers)
View(black)
View(blackpaperLCCN)
View(black_papers)
x %>%
count(black)
x <- jackindex %>%
left_join(black_papers, by=c("sn"="LCCN"))
x <- x %>%
select(file_id, article_id, sn, newspaper_name, year, month, day, page, edition, mod_id, URL, newspaper_state, filename,  filepath, black) %>%
filter(black=="Y")
blackLOC<- jackindex %>%
left_join(black_papers, by=c("sn"="LCCN"))
blackLOC <- blackLOC %>%
select(file_id, article_id, sn, newspaper_name, year, month, day, page, edition, mod_id, URL, newspaper_state, filename,  filepath, black) %>%
filter(black=="Y")
View(blackLOC)
df <- blackLOC$filename
head(df)
filestocopy <- list.files(inputdir, full.names = TRUE)
inputdir  <- "/Users/robwells/Code/hcij_lynching_phase_two/articles_cleaned_2023_03_08"
targetdir <- "/Users/robwells/Code/hcij_lynching_phase_two/narratives/code/blackLOC"
df <- blackLOC$filename
filestocopy <- list.files(inputdir, full.names = TRUE)
filestocopy <- unique(grep(paste(df,collapse="|"), filestocopy, value=TRUE))
sapply(filestocopy, function(x) file.copy(from=x, to=targetdir, copy.mode = TRUE))
write.csv(blackLOC, "../blackLOC/blackLOC_May2.csv")
write.csv(blackLOC, "/blackLOC/blackLOC_May2.csv")
write.csv(blackLOC, "../blackLOC/blackLOC_May2.csv")
write.csv(blackLOC, "../output/blackLOC_May2.csv")
shiny::runApp('Code/test_class')
runApp('Code/test_class')
setwd("~/Code/test_class")
runApp()
devtools::install_github("daattali/shinyforms")
install.packages("devtools")
devtools::install_github("daattali/shinyforms")
library(shiny)
library(shinyforms)
questions <- list(
list(id = "name", type = "text", title = "Name", mandatory = TRUE),
list(id = "age", type = "numeric", title = "Age"),
list(id = "favourite_pkg", type = "text", title = "Favourite R package"),
list(id = "terms", type = "checkbox", title = "I agree to the terms")
)
formInfo <- list(
id = "basicinfo",
questions = questions,
storage = list(
# Right now, only flat file storage is supported
type = STORAGE_TYPES$FLATFILE,
# The path where responses are stored
path = "responses"
)
ui <- fluidPage(
formUI(formInfo)
)
server <- function(input, output, session) {
formServer(formInfo)
}
shinyApp(ui = ui, server = server)
runApp('app2.R')
install.packages("devtools")
runApp('app2.R')
install.packages("devtools")
runApp('app2.R')
library(googlesheets4)
runApp('app2.R')
install.packages("devtools")
runApp('app2.R')
install.packages("devtools")
library(shiny); runApp('app2.R')
install.packages("devtools")
# Create a new google sheets file
df <- data.frame(name = "", age = 0, favourite_pkg = "", terms = TRUE)
google_df <- gs_new("responses", input = df, trim = TRUE, verbose = FALSE)
library(googlesheets)
install.packages("googlesheets")
View(df)
google_df <- read_sheet("https://docs.google.com/spreadsheets/d/1w7Nl8PfRNopjUPmw763TMLECAY_9Ehtn2Q80t9l57-0/edit#gid=1436302326")
google_df <- rio::import("https://docs.google.com/spreadsheets/d/1w7Nl8PfRNopjUPmw763TMLECAY_9Ehtn2Q80t9l57-0/edit#gid=1436302326")
View(google_df)
google_df <- rio::import("https://docs.google.com/spreadsheets/d/1w7Nl8PfRNopjUPmw763TMLECAY_9Ehtn2Q80t9l57-0/edit?usp=sharing")
View(google_df)
google_df <- read_sheet("https://docs.google.com/spreadsheets/d/1w7Nl8PfRNopjUPmw763TMLECAY_9Ehtn2Q80t9l57-0/edit?usp=sharing")
google_df <- rio::import("https://docs.google.com/spreadsheets/d/1w7Nl8PfRNopjUPmw763TMLECAY_9Ehtn2Q80t9l57-0/edit?usp=sharing")
View(google_df)
google_df <- read_sheet("https://docs.google.com/spreadsheets/d/1w7Nl8PfRNopjUPmw763TMLECAY_9Ehtn2Q80t9l57-0/edit?usp=sharing")
google_df <- rio::import("https://docs.google.com/spreadsheets/d/1w7Nl8PfRNopjUPmw763TMLECAY_9Ehtn2Q80t9l57-0/edit?usp=sharing")
View(google_df)
google_df <- rio::import("https://docs.google.com/spreadsheets/d/1w7Nl8PfRNopjUPmw763TMLECAY_9Ehtn2Q80t9l57-0/edit?usp=sharing")
View(google_df)
google_df <- rio::import("https://docs.google.com/spreadsheets/d/1w7Nl8PfRNopjUPmw763TMLECAY_9Ehtn2Q80t9l57-0/edit?usp=sharing")
google_df <- read_sheet("https://docs.google.com/spreadsheets/d/1w7Nl8PfRNopjUPmw763TMLECAY_9Ehtn2Q80t9l57-0/edit?usp=sharing")
google_df <- rio::import("/Users/robwells/Downloads/mob_analysis test - mob_analysis_jan_25.csv")
View(google_df)
names(google_df)
runApp('app2.R')
install.packages("devtools")
